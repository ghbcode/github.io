{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "<br><br>\n",
    "Random Forest is an ensembble method that uses bagging (bootstrap aggregating). Various CART models are built and they are averaged, or perhaps majority voting is used to arrive at the estimate. \n",
    "\n",
    "It helps to first consider what are the pros and cons of Decision Trees:<br>\n",
    "- Pros\n",
    "  - Adaptable to different functions. f(x) does not have to be a continuous and differentiable function\n",
    "  - Useful for classification and regression\n",
    "  - Can give feature importance/feature selection\n",
    "  - It is tractable. You can show someone the decision tree and why a certain sample ended up at a particular leaf\n",
    "- Cons\n",
    "  - Give a piecewise constant approximation that is not differentiable (refer to Murphy Figure 16.1b)\n",
    "  - Can easily overfit if not careful\n",
    "  - Unstable to slight changes in the data. A slight change in data can completely alter the 'path'\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Random Forest reduces the variance of the estimates and this is accomplished by two aspects of RF that are described below.<br>\n",
    "\n",
    "- Draw many data sets D_training with replacement\n",
    "- Before splitting region $R_m$ select a random subset of features, d, where d < D (D is the full set of features)\n",
    "  - select the best feature in d to threshold\n",
    "    - Here the similarity between trees is small\n",
    "    - When the trees are averaged or voted on, this low similarity reduces variance considerably\n",
    "  - This gives rise to a single CART, $T_m, m \\in [1,M]$, and an estimate $\\hat{f_m}(x)$  \n",
    "- Final step averages all of the trees to arrive at an estimate. Voting can also be used instead of averaging.\n",
    "  - For regression the result is $$\\hat{f}(x) = \\frac{1}{M}\\sum_{m=1}^M \\hat{f_m}(x) $$\n",
    "    - This is referred to as bagging \n",
    "    - This averaging is one of the ways the Random Forests reduce variance\n",
    "  - For classification the result is $$\\hat{y}(\\vec{x}) = \\underset{c}{\\mathrm{argmax}}\\quad\\sum_{b=1}^B(\\hat{y}^{(b)} = c)$$\n",
    "<br>where the classification of a region is done by vote, $\\hat{y}$ is the winning class<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Test set accuracy (% correct) when max_depth = 5: 0.607\n",
      "Random Forest: Test set accuracy (% correct) when max_depth = 50: 0.596\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this example we are reading in a house description and sale dataset. For this classification we are going to \n",
    "estimate whether a house will sell(and with what probability) within 90 days of being put on the market.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this data has already been cleaned up, standardized, one hot encoded and vetted\n",
    "df = pd.read_csv(\"classification_house_sale_px_data.csv\", parse_dates=True, sep=',', header=0)\n",
    "df_labels = pd.read_csv(\"classification_house_sale_px_labels.csv\", parse_dates=True, sep=',', header=0)\n",
    "\n",
    "# split data into training and test sets\n",
    "train, test, y_train, y_test = train_test_split(df, df_labels, train_size=.6, test_size=.4, shuffle=True)\n",
    "\n",
    "# run the classifier on the training data\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "# make prediction on the test data\n",
    "#predicted = clf.predict(test)\n",
    "print(\"Random Forest: Test set accuracy (% correct) when max_depth = 5: {0:.3f}\".format(clf.score(test, y_test.label.values)))\n",
    "# run the classifier on the training data\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "print(\"Random Forest: Test set accuracy (% correct) when max_depth = 50: {0:.3f}\".format(clf.score(test, y_test.label.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Note how the RF is betwer than the Decision Tree, albeit by a small margin. Even with a higher depth the results are better than the Decision Tree. \n",
    "<br>\n",
    "# Take away\n",
    "- Random forest is an ensemble method that averages the results of many decision trees (CART)\n",
    "- Random forest reduces variance due to the averaging and bagging calculations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
