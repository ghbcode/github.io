{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees (CART)\n",
    "<br><br>\n",
    "First it needs to be noted that CART stands for Classification and Regression Tree and it is the same as a decision tree. Also, CART can be used for either classification or regression. At its core the decision tree makes binary decisions on the data by picking the feature and its value that will increase the cost function the least amount possible. For classification the cost function is: \n",
    "<br>\n",
    "$$ cost(x_i, y_i) \\in R_{m'} = \\frac{1}{N_{R_m}} \\sum_{x_i \\in R_{m'}} \\mathbb{1}[y_i \\ne \\hat{y_i}(R_{m'})]$$\n",
    "<br><br>\n",
    "And for regression the cost function is:<br>\n",
    "$$ cost(x_i, y_i) \\in R_{m'} = \\sum_{x_i \\in R_{m'}} (y_i - w_{m'})^2$$\n",
    "<br><br>\n",
    "So using the ubiquitous iris classification example, the figure below on the left shows a sample set of decisions made in a particular tree. At each iteration a so called 'greedy algorithm' is used to minimize the cost function and thereby determining the specific split. On the right of the figure you can see how the feature space is partitioned into blocks that are parallel to the features. If you follow the decision tree on the left you will see how the figure on the right is derived. \n",
    "\n",
    "![l1 and l1 regularization](cart.png)Figure taken from Machine Learning A Probabilistic Perspective, Kevin P. Murphy\n",
    "<br><br>\n",
    "You could create a tree that exhaustively partitions the data so that there is a single training data point at every leaf. But of course that would be tantamount to over fitting and may very well be computationally wasteful. Therefore a **stopping condition** is usually defined that could seek a minimum number of data points per leaf/node (say 10). Lastly, you could also **prune** the tree to further increase performance. There are several methods to prune a tree but a common one is to remove a leaf/node if doing so reduces the cost function on the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART: Test set accuracy (% correct) when max_depth = 5: 0.600\n",
      "CART: Test set accuracy (% correct) when max_depth = 50: 0.558\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this example we are reading in a house description and sale dataset. For this classification we are going to \n",
    "estimate whether a house will sell(and with what probability) within 90 days of being put on the market.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this data has already been cleaned up, standardized, one hot encoded and vetted\n",
    "df = pd.read_csv(\"classification_house_sale_px_data.csv\", parse_dates=True, sep=',', header=0)\n",
    "df_labels = pd.read_csv(\"classification_house_sale_px_labels.csv\", parse_dates=True, sep=',', header=0)\n",
    "\n",
    "# split data into training and test sets\n",
    "train, test, y_train, y_test = train_test_split(df, df_labels, train_size=.6, test_size=.4, shuffle=True)\n",
    "\n",
    "# run the classifier on the training data\n",
    "clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "# make prediction on the test data\n",
    "#predicted = clf.predict(test)\n",
    "print(\"CART: Test set accuracy (% correct) when max_depth = 5: {0:.3f}\".format(clf.score(test, y_test.label.values)))\n",
    "# run the classifier on the training data\n",
    "clf = DecisionTreeClassifier(max_depth=50, min_samples_leaf=5)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "print(\"CART: Test set accuracy (% correct) when max_depth = 50: {0:.3f}\".format(clf.score(test, y_test.label.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Note how a deeper tree leads to overfitting as can be seen by the lower score (% correct) when max_depth is set to 50.\n",
    "<br>\n",
    "\n",
    "# Take away\n",
    "- CART is a Decision Tree\n",
    "- CART makes use of binary decisions \n",
    "- CART can be improved by pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
