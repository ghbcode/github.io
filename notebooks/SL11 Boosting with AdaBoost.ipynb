{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting with AdaBoost\n",
    "<br>\n",
    "Boosting is another ensemble method that is also an adaptive basis function model, i.e. the basis function, $\\phi(x)$ does not have to be linear in the parameters. In short, the estimate $\\hat{f}(x)$ is defined by:\n",
    "$$\\hat{f}(x) = w_0 + \\sum_{m=1}^m w_m\\phi(x,\\gamma_m)$$ \n",
    "- where each $\\phi(x,\\gamma_m)$\n",
    "  - is a simple classifier that can classify the entire feature space\n",
    "  - is a weak learner that is only required to do better than chance\n",
    "    - each weak learner is a 'stump', i.e. a 1 stage CART with one node and two leaves\n",
    "\n",
    "For a two class problem where $\\tilde{y} \\in \\{-1, 1\\}$\n",
    "$$\\hat{f}(x) = f_0 + \\sum_{m=1}^m \\beta_m\\phi(x,\\gamma_m)$$ \n",
    "where \n",
    "- $\\beta_m$ is the \"importance\" of the $m^{th}$ classifier\n",
    "- $\\phi(x,\\gamma_m)$ is the $m^{th}$ base classifier\n",
    "\n",
    "and the final classifier is defined by:\n",
    "<br>\n",
    "$$\\hat{y}(x) = sign\\{\\hat{f}(x)\\}$$\n",
    "<br>\n",
    "And |$\\hat{f}(x)$| provides a measure of confidence in the class assignment of x. It is worth stressing that Boosting with each iteration is trying to improve on the estimate and that is where $\\beta_m$ comes from. In the image below (credit goes to analyticsvidhya.com) you can appreciate how boosting works. In the first iteration illustrated by 'box 1' you can see that the split misclassified the three '+' samples toward the top. So these three '+' samples gain more importance as can be appreciated in 'box 2' since they appear larger. Now in the second iteration the three '-' samples to the left of the split are misclassified so that in box 3 they appear larger (and the others appear smaller). Finally, the box in the bottom of the graphic shows the aggregated results. \n",
    "\n",
    "![Boosting](boosting.png)\n",
    "<br>\n",
    "An important aspect in the Boosting procedure is how to calculate $\\phi_m(x)$ and for that we use the following objective function:\n",
    "<br>\n",
    "\n",
    "$$ f_{obj} = [\\{\\beta_m, m=1,...,M\\}, \\{\\gamma_m, m=1,...,M\\}, \\phi(x)]\\\\\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N}L(\\tilde{y_i}, f(x_i))$$\n",
    "<br>\n",
    "where L is a loss function such as:\n",
    "- the 0-1 loss function, $L_{0-1} = \\mathbb{I}(\\tilde{y_i} \\ne sign\\{\\hat{f}(x_i)\\})$\n",
    "- the exponential loss function, $L_{exp}(\\tilde{y_i},\\hat{f}(x_i)) = exp(-\\tilde{y_i}\\hat{f}(x_i)) $\n",
    "<br><br>\n",
    "\n",
    "So you want to find:<br>\n",
    "$$\\hat{f}(x) = \\underset{f(x)}{\\mathrm{argmin}}\\quad \\sum_{i=1}^N L_{exp}(\\tilde{y_i},\\hat{f}(x_i))$$\n",
    "$$= \\underset{f_0, \\beta_m, \\gamma_m}{\\mathrm{argmin}}\\quad \\sum_{i=1}^N L_{exp}(\\tilde{y_i}, f_0 + \\sum_{m=1}^M \\beta_m\\phi(x,\\gamma_m))$$\n",
    "<br><br>\n",
    "\n",
    "AdaBoost is a Boosting method where the loss function is $L_{exp}(\\tilde{y_i},\\hat{f}(x_i))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: Test set accuracy (% correct) when n_estimators = 5: 0.615\n",
      "AdaBoost: Test set accuracy (% correct) when n_estimators = 20: 0.630\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this example we are reading in a house description and sale dataset. For this classification we are going to \n",
    "estimate whether a house will sell(and with what probability) within 90 days of being put on the market.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this data has already been cleaned up, standardized, one hot encoded and vetted\n",
    "df = pd.read_csv(\"classification_house_sale_px_data.csv\", parse_dates=True, sep=',', header=0)\n",
    "df_labels = pd.read_csv(\"classification_house_sale_px_labels.csv\", parse_dates=True, sep=',', header=0)\n",
    "\n",
    "# split data into training and test sets\n",
    "train, test, y_train, y_test = train_test_split(df, df_labels, train_size=.6, test_size=.4, shuffle=True)\n",
    "\n",
    "# run the classifier on the training data\n",
    "clf = AdaBoostClassifier(n_estimators=5)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "# make prediction on the test data\n",
    "#predicted = clf.predict(test)\n",
    "print(\"AdaBoost: Test set accuracy (% correct) when n_estimators = 5: {0:.3f}\".format(clf.score(test, y_test.label.values)))\n",
    "# run the classifier on the training data\n",
    "clf = AdaBoostClassifier(n_estimators=20)\n",
    "clf.fit(train, list(y_train.label.values))\n",
    "print(\"AdaBoost: Test set accuracy (% correct) when n_estimators = 20: {0:.3f}\".format(clf.score(test, y_test.label.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Note how the AdaBoost estimate is as good as the best decision tree estimate, however, the worst AdaBoost estimate does not degrade as much as the worst decision tree estimate.  \n",
    "<br>\n",
    "# Take away\n",
    "- Boosting is an ensemble method that makes use of weak learners and aggregates those results for final estimate\n",
    "- Boosting takes the previous iteration results and attempts to improve on them by assining a higher importance to previously misclassified samples\n",
    "- AdaBoost uses a CART weak learner and the $L_{exp}$ loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
