{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Supervised Learning\n",
    "<br>\n",
    "When you have labeled and unlabeled data you might as well use the unlabeled data to increase the predictive strength of your model. In this case you will make the following assumptions:\n",
    "<br>\n",
    "* The labeled samples are representative of the data (i.e. they are not outliers)\n",
    "* All instances are drawn from an i.i.d. distribution\n",
    "<br>\n",
    "\n",
    "So that $p(x) = \\sum_y p(x|y)p(y)$ \n",
    "<br>\n",
    "\n",
    "There are two types of semi-supervised learning:\n",
    "<br>\n",
    "\n",
    "* **inductive** - continually learns from data, i.e. learns $\\hat{y} = \\hat{f}(x)$ over all feature space X\n",
    "* **transductive** - predicts the labels on the unlabeled instances of the training data. So it learns $\\hat{y_j} = \\hat{f}(x_j \\forall x_j \\in D_U)$\n",
    "<br>\n",
    "\n",
    "### Self training algorithms\n",
    "Use labeled data to find $\\hat{f}(x^{(U)})$. Then use/consider $\\hat{f}(x^{(U)})$ as \"labeled\" data. Some examples include:\n",
    "<br>\n",
    "  * **Self training algorithm** - The algorithm takes the highest confidence estimate(s) in the unlabeled data and moves it(them) to the labeled data set, thereby iteratively increasing the labeled data set. \n",
    "  * **Propagating 1-NN** - Use a distance function to iteratively merge the unlabeled data to the labeled data set.\n",
    "    * If you imagine labeled and unlabeled points in 2D, this algorithm operates on the closest pair consisting of an unlabled and a labeled point. The unlabeled data point is absorbed into the labeled set and the process repeats.\n",
    "    * This algorithm works well when the data is dense and well seprated for each class. \n",
    "<br><br>\n",
    "\n",
    "### Mixture Models\n",
    "Note that [the following video](https://www.youtube.com/watch?v=REypj2sy_5U&list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt) has a great explanation of mixture models and their use. Recall that there are (at least) two types of clustering methods. \n",
    "* You have hard clustering where an element either belongs to a cluster or not. \n",
    "* You have soft clustering where an element may be associated with different clusters albeit with differing degrees of confidence.\n",
    "<br>\n",
    "\n",
    "So given data that has a higher kurtosis than the Gaussian distribution or if it appears multimodal, then chances are that the data is comprised of multiple classes. Mixture models are a probabilistically sound way of doing soft clustering with the Gaussian Mixture Model (GMM) being a common choice.\n",
    "![multimodal distribution](multimodal.jpg)\n",
    "<br>\n",
    "\n",
    "Goal: We want to find $p(y|\\underline{x})$<br>\n",
    "Process: \n",
    "  * Model each class as a specified density with unknown parameters\n",
    "  $$p(\\underline{x},y|\\underline{\\theta}) = p(\\underline{x}|y,\\underline{\\theta})p(y|\\underline{\\theta})\\\\\n",
    "  = p(\\underline{x}|y,\\underline{\\theta})p(y)\\\\\n",
    "  = \\text{class-conditional density * prior}$$\n",
    "  <br>\n",
    "  \n",
    "  * for the labeled data we have $p(\\underline{x}|y=1,\\underline{\\theta})$ and $p(\\underline{x}|y=2,\\underline{\\theta})$\n",
    "  <br>\n",
    "  ![mixture model](mixture-model.jpg)\n",
    "  <br>\n",
    "  \n",
    "  * but for the unlabeled data you know know the class for y so we have\n",
    "  $$p(\\underline{x}|\\underline{\\theta}) = \\sum_{y=1}^C p(\\underline{x}|y,\\underline{\\theta})p(y|\\underline{\\theta})\\\\\n",
    "  =\\sum_{y=1}^C p(\\underline{x}|y,\\underline{\\theta})p(y)\\\\\n",
    "  = \\text{component densities * mixing parameters}\n",
    "  $$\n",
    "  <br>\n",
    "  \n",
    "Here $p(\\underline{x}|\\underline{\\theta})$ is a mixture density. But how do we use the labeled and unlabeled data together? We can find $\\underline{\\theta}$ from the data using MLE. \n",
    "<br>\n",
    "$$\\hat{\\theta_{MLE}} = \\underset{\\theta}{\\mathrm{argmax}}\\quad p(D|\\theta)\\\\\n",
    "= \\underset{\\theta}{\\mathrm{argmax}} \\space ln[p(D|\\theta)]\\\\\n",
    "= \\underset{\\theta}{\\mathrm{argmax}} \\sum_{i=1}^{l} ln[p(x_i,y_i|\\theta)] + \\sum_{i=l+1}^{l+u} ln[p(x_i|\\theta]\\\\\n",
    "= \\underset{\\theta}{\\mathrm{argmax}} \\sum_{i=1}^{l} ln[p(x_i|y_i\\theta) + ln[p(y_i)] + \\sum_{i=l+1}^{l+u} ln[\\sum_{y=1}^C p(\\underline{x}|y,\\underline{\\theta})p(y)]\n",
    "$$\n",
    "<br>\n",
    "where:<br>\n",
    "\n",
    "- the labeled data goes from i=1 to l, and the unlabeled data goes from i=l+1 to l+u\n",
    "- $ p(y_i|\\theta)=p(y_i)$ since $\\theta$ doesn't give you information about y \n",
    "- remember that for the unlabeled data $p(\\underline{x}|\\underline{\\theta})=\\sum_{y=1}^C p(\\underline{x}|y,\\underline{\\theta})p(y)$\n",
    "<br>\n",
    "\n",
    "Finally, remember that the classes as well as the parameters $\\underline{\\theta}$ are unknown and that you need one to estimate the other. So how do you estimtate the classes and the parameters? That is where expectation maximization (EM) comes into the picture.\n",
    "<br>\n",
    "\n",
    "### Expectation Maximization\n",
    "\n",
    "Let's begin by listing below the comon uses for EM. Note that the second one is exactly what we are trying to figure out in the mixture model above. \n",
    "* Finding missing data\n",
    "* Find MLE in difficult situations\n",
    "* Estimate quantities in mixture models\n",
    "<br>\n",
    "\n",
    "Let *D* be all of the data where *D* = {$(x_i,y_i), i=1,2,...,l; x_h, h=l+1, l+2,...,l+u$} and *H* = {$y_h, h=l+1, l+2,...,l+u$} are the 'hidden' labels. Here is how the EM algorithm works:\n",
    "1. initialize t=0 (iteration index) and $\\underline{\\theta}^0$. Theta from the labeled data is a good starting point for $\\underline{\\theta}^0$\n",
    "2. Compute the best estimate of *H* as $p(H|D, \\underline{\\theta}^{(t)})$\n",
    "3. Estimate parameter $\\underline{\\theta}^{(t+1)}$ by \n",
    "    $$\\theta^{(t+1)} = \\underset{\\theta}{\\mathrm{argmax}} \\quad E_{H|D,\\underline{\\theta^{(t)}}}\\{ ln[p(D,H|\\underline{\\theta}]\\} $$\n",
    "<br>\n",
    "\n",
    "where steps 2 and 3 are repeated in succession until convergence is reached or a halting condition is reached. Note that EM has the following properties:<br>\n",
    "- Can be shown that p(D|$\\theta$) increases at each iteration\n",
    "- Converges to **local optimum**\n",
    "- The result depends on the starting point of $\\underline{\\theta}^0$. As mentioned previously, a common choice is $\\hat{\\underline{\\theta}}_{MLE}$ using the labeled data set.\n",
    "<br>\n",
    "\n",
    "So the EM algorithm requires a good starting point and if it receives one it should iteratively get closer to discerning the classes and parameters for the unlabeled data set. The graphic below should give an intuition for EM.\n",
    "\n",
    "![expectation maximization](https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "* Great video on EM: https://www.youtube.com/watch?v=iQoXFmbXRJA&index=2&list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
